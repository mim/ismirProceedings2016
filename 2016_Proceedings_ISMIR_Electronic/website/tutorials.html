<html>
<head>
<meta charset="utf-8">
<link rel="stylesheet" href="assets/bower_components/bootstrap/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="assets/css/style.css">
<script src="assets/bower_components/jquery/dist/jquery.js"></script>
<script src="assets/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
<title>ISMIR 2015 - Tutorials</title>
</head>
<body>

<div class="container">
    <div class="row page-header">
        <div class="col-md-3">
            <a href="../index.html"><img class="img-responsive" src="assets/img/logo.png" /></a>
        </div>
        <div class="col-md-9">
            <h1>
                <p>
                    Proceedings of the<br />
                    <strong>16th International Society for Music Information Retrieval Conference</strong><br />
                </p>
                
                <p>
                    October 26-30, 2015<br />
                    Málaga, Spain
                </p>
                Edited by Meinard Müller and Frans Wiering
            </h1>
        </div>
    </div>
  <div class="container">
      <h2 id="tut1">Tutorial 1: Why Singing is Interesting</h2>
      <ul>
        <li>Simon Dixon (Queen Mary University of London, UK)</li>
        <li>Masataka Goto (AIST, Japan)</li>
        <li>Matthias Mauch (Queen Mary University of London, UK)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        This tutorial aims to introduce to the ISMIR community the exciting world of singing styles,
        the mechanisms of the singing voice, and provide a guide to representations, engineering
        tools, and methods for analyzing and leveraging it. The singing voice is arguably the most
        expressive of all musical instruments, and all popular music cultures around the world use
        singing. Across disciplines, a lot is known about singing culture and the intricate
        physiological and psychological mechanisms of singing, but this knowledge is not exploited
        enough in much of the music information retrieval literature. The three parts of the tutorial
        (one hour each) are designed to remedy this: an introduction to singing styles, techniques and
        forms around the world (including a short introduction to the psychology of singing), a
        practical guide to the analysis of singing using music informatics tools, and an overview over
        various systems for singing information processing. Our aim is for music information
        retrieval specialists to walk away with a newly sparked passion for singing, and ideas of how
        to use our knowledge of singing, and singing information processing, to create new, exciting
        research.
      </p>
      
      <h2 id="tut2">Tutorial 2: Addressing the Music Information Needs of Musicologists</h2>
      <ul>
        <li>Richard J. Lewis (Goldsmiths College, University of London, UK)</li>
        <li>Ben Fields (Goldsmiths College, University of London, UK)</li>
        <li>Tim Crawford (Goldsmiths College, University of London, UK)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        The music information needs of musicologists are not being met by the current generation of
        MIR tools and techniques. While evaluation has always been central to the practice of the
        music information retrieval community, the tasks tackled most often address the music
        information needs of recreational users, such as playlist recommendation systems; or are
        specified at a level which is not very relevant to the needs of music researchers, such as beat
        or key finding; or have focused on―and possibly even become over-fitted to―a narrow
        range of musical repertoire which doesn't cover musicological interests. In this tutorial we
        will present those music information needs through topics including at least the following:
        the metadata requirements of historical musicology; working with symbolic corpora;
        studying musical networks; passage-level audio search; and musical understandings of audio
        features. As well as these scheduled presentations and discussions, we will ask the attendees
        to submit suggestions of musicologically motivated research questions suitable for MIR
        during the course of the tutorial. These will then be reviewed and discussed during the
        conclusion of the tutorial. Finally, we have invited Meinard Müller to conclude the tutorial by
        outlining his view on the current state of MIR for musicology. We are aiming to enable
        attendees, as experts in their own areas of MIR, to find new applications of their tools and
        techniques that can also serve the needs of musicologists. Given the selection of MIR topics
        we intend to cover, this tutorial will be of particular interest to those working in: musical
        metadata; symbolic MIR; audio search; and graph analytics. We believe contemporary
        musicology to be a rich source of new and exciting challenges for MIR and we are confident
        the community can rise to those challenges. In the long term, we hope this tutorial will give
        rise to a selection of new MIREX tasks that focus on musicological challenges.
      </p>

      <h2 id="tut3">Tutorial 3: Markov Logic Networks for Music Analysis</h2>
      <ul>
        <li>Helene Papadopoulos (CNRS, Paris, France)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        The automatic extraction of relevant content information from music audio signals is an
        essential aspect of Music Information Retrieval (MIR). Music audio signals are very rich and
        complex, both because of the intrinsic physical nature of audio (incomplete and noisy
        observations, many modes of sound production, etc.), and because they convey multi-faceted
        and strongly interrelated semantic information (harmony, melody, metric, structure, etc.).
        Dealing with real audio recordings thus requires the ability to handle both uncertainty and
        complex relational structure at multiple levels of representation. Until recent years, these two
        aspects have been generally treated separately, probability being the standard way to
        represent uncertainty in knowledge, while logical representation being used to represent
        complex relational information. Markov Logic Networks (MLNs), in which statistical and
        relational knowledge are unified within a single representation formalism, have recently
        received considerable attention in many domains such as natural language processing,
        link-based Web search, or bioinformatics. The goal of this tutorial is to provide a
        comprehensive overview of Markov logic networks and show how they can be used as a
        highly flexible and expressive yet concise formalism for the analysis of music audio signals.
        We will show how MLNs encompass the probabilistic and logic-based models that are
        classically used in MIR. Algorithms for MLN modeling, training and inference will be
        presented, as well as open-source software packages for MLNs that are suitable to MIR
        applications. We will discuss concrete case-study examples in various fields of application.
      </p>
      
      <h2 id="tut4">Tutorial 4: COmputation and FLAmenco: Why Flamenco is Interesting for MIR Research</h2>
      <ul>
        <li>Emilia Gómez (Universitat Pompeu Fabra, Barcelona, Spain)</li>
        <li>Nadine Kroher (Universitat Pompeu Fabra, Barcelona, Spain)</li>
        <li>Jose Miguel Díaz-Báñez (Universidad de Sevilla, Spain)</li>
        <li>Sergio Oramas (Universitat Pompeu Fabra, Barcelona, Spain)</li>
        <li>Joaquín Mora (Universidad de Sevilla, Spain)</li>
        <li>Francisco Gómez-Martín (Universidad Politécnica de Madrid, Spain)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        This tutorial provides an introduction to flamenco music with the support of MIR techniques.
        At the same time, the tutorial analyzes the challenges and opportunities that this music
        repertoire offers MIR researchers, presents some research contributions and provides a forum
        to discuss about how to address those challenges in future research. As ISMIR 2015 is in
        Málaga, this tutorial will give ISMIR participants a unique chance to discover flamenco
        music in its original location. The tutorial will be structured in two main parts. First, we will
        provide a general introduction to flamenco music: origins and evolution, musical
        characteristics, instrumentation, singing and guitar. We will illustrate this introduction with
        multimedia material and live performance. Then we will analyze how MIR technologies
        perform for flamenco music. By discussing several MIR tasks and how they should be
        addressed in this context, we will discover more about flamenco and how methods tailored to
        this repertoire can be exploited in other contexts. We will focus on automatic transcription,
        singer identification, music similarity, genre classification, rhythmic and melodic pattern
        detection and context-based music description methods. Participants will have the chance to
        interact with MIR annotated datasets and tools developed for flamenco music in the context
        of the COFLA project.
      </p>

      <h2 id="tut5">Tutorial 5: Using Correlation Analysis and Big Data to Identify and Predict Musical Behaviors</h2>
      <ul>
        <li>Jeff C. Smith (Smule)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        New and significant repositories of musical data afford unique opportunities to apply data
        analysis techniques to ascertain insights of musical engagement. These repositories include
        performance, listening, curation, and behavioral data. Often the data in these repositories also
        includes demographic and/or location information, allowing studies of musical behavior, for
        example, to be correlated with culture or geography. Historically, the analysis of musical
        behaviors was limited. Often, subjects (e.g. performers or listeners) were recruited for such
        studies. This technique suffered from issues around methodology (e.g. the sample set of
        subjects would often exhibit bias) or an insufficient number of subjects and/or data to make
        reliable statements of significance. That is to say the conclusions from these studies were
        largely anecdotal. In contrast to these historical studies, the availability of new repositories of
        musical data allow for studies in musical engagement to develop conclusions that pass
        standards of significance, thereby yielding actual insights into musical behaviors. This
        tutorial will demonstrate several techniques and examples where correlation and statistical
        analysis is applied to large repositories of musical data to document various facets of musical
        engagement. Web site: https://ccrma.stanford.edu/damp/ Stanford University has created a
        new corpus of amateur music performance data, the Stanford Digital Archive of Mobile
        Performances, or DAMP, to facilitate the study of musical engagement through application of
        correlation and statistical analysis.
      </p>
  
      <h2 id="tut6">Tutorial 6: Automatic Music Transcription</h2>
      <ul>
        <li>Zhiyao Duan (University of Rochester, USA)</li>
        <li>Emmanouil Benetos (Queen Mary University of London, UK)</li>
      </ul>
      <h3>Abstract</h3>
      <p>
        Automatic Music Transcription (AMT) is a fundamental problem in music information
        retrieval. Roughly speaking, transcription refers to extracting a symbolic representation —a
        list of notes (pitches and rhythms)—from an audio signal. Music transcription is a fascinating
        but challenging task, even for humans: in undergraduate music education it is usually called
        dictation, and achieving a high level of proficiency requires years of practice and training.
        Empowering machines with this ability is an even more challenging problem, especially for
        automatically transcribing polyphonic music. To that end, the AMT problem has drawn great
        interest of researchers from several areas including signal processing, machine learning,
        acoustics, music theory, and music cognition. In terms of applications, a successful AMT
        system would be helpful for solving many MIR research problems, including music source
        separation, structure analysis, content-based music retrieval, and musicological study of
        non-notated music, just to name a few. This tutorial will give an overview of the AMT
        problem, including current approaches, datasets and evaluation methodologies. It will also
        explore connections with other related problems (i.e. audio-score alignment, source
        separation) as well as applications to related fields, such as content-based music retrieval and
        computational musicology. The tutorial is designed for students and researchers who have
        general knowledge of music information retrieval and/or computational musicology and are
        interested in getting into the field of AMT. A substantial amount of time will be spent in
        discussing challenges and research directions; we hope that this discussion will help move
        this field forward, and influence related fields in MIR and computational musicology to
        exploit AMT technologies. The tutorial will also include hands-on sessions on using AMT
        code and plugins - participants will be encouraged to bring their laptops and gain access to
        transcription datasets, as well as work on AMT examples.
      </p>
  </div>
</div>

</body>
</html>
